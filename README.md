# Local RAG Playground â€” A Microlearning Guide

Welcome to the Local RAG Playground â€” a handsâ€‘on environment designed to help you understand how Retrievalâ€‘Augmented Generation (RAG) works using Python, Ollama, and pgvector.

This project is intentionally simple, transparent, and educational.
Everything runs locally, so you can explore RAG concepts without external APIs or cloud dependencies.

## ğŸ§© What This Project Is

This project is a lightweight, fully local Retrievalâ€‘Augmented Generation (RAG) system built with:

- **Python** â€” Core language
- **Ollama** â€” Local LLM + Embeddings engine
- **PostgreSQL + pgvector** â€” Vector database
- **LangChain** â€” Text splitting & orchestration
- **Jupyter** â€” Interactive experimentation notebooks

### You Can:
- Ingest documents (PDFs, resumes, knowledge files)
- Chunk them into semantically meaningful pieces
- Embed them using multilingual embedding models
- Store embeddings in a vector database
- Perform semantic search over your knowledgebase
- Generate answers grounded in your documents

## ğŸš€ Features

| Feature | Benefit |
|---------|---------|
| ğŸ“„ PDF ingestion with text extraction | Easy document loading |
| âœ‚ï¸ Semantic chunking | Preserves context & meaning |
| ğŸ§  Multilingual embeddings | Works across languages |
| ğŸ” Vector similarity search | Find relevant content fast |
| ğŸ§± Modular architecture | Easy to understand & extend |
| ğŸ§ª Jupyter notebooks | Learn interactively |
| ğŸ”’ Fully local | No external APIs, complete privacy |


## ğŸ§± Architecture Overview

### The RAG Pipeline

```
Ingestion Phase:
  PDF â†’ PdfChunker â†’ Semantic Chunks
                           â†“
                    Embedding (Ollama)
                           â†“
                     pgvector (Store)

Retrieval Phase:
  User Query â†’ Embedding (Ollama) â†’ Vector Search â†’ Top K Chunks â†’ LLM â†’ Answer
```

### Data Flow Example:

```
1. Upload file.pdf
   â””â”€ Extract text
   â””â”€ Split into chunks (preserving context)
   â””â”€ Convert each chunk to embedding vector
   â””â”€ Store in PostgreSQL with pgvector

2. User asks: "What are the candidate's skills?"
   â””â”€ Convert question to embedding
   â””â”€ Search pgvector for similar chunks (cosine distance)
   â””â”€ Retrieve top 3 matching chunks
   â””â”€ Pass to LLM: "Based on these chunks, answer the question"
   â””â”€ Return grounded answer
```

## ğŸ§© Core Components

| Component | Purpose |
|-----------|---------|
| **Ollama** | Provides embeddings + LLM generation locally |
| **PdfChunker** | Extracts text from PDFs, intelligently chunks content |
| **Embedding Engine** | Converts text to 1024-dim vectors |
| **PostgreSQL + pgvector** | Stores vectors, performs similarity search |
| **RagController** | Orchestrates ingestion, search, generation |
| **SQLAlchemy Models** | ORM for documents, chunks, embeddings |

Each component is intentionally small and easy to understand.

## âš™ï¸ System Requirements

### Hardware

- **RAM**: 8 GB minimum (16 GB recommended for smooth operation)
- **Disk**: 10 GB+ for models and data
- **CPU**: Modern processor (LLMs work on CPU, but benefit from adequate cores)

### Software

- **Python 3.10+**
- **PostgreSQL 13+** with pgvector extension
  - Vector column: `embedding vector(1024)`
- **Ollama** installed and running locally
- **Docker** (optional, for containerized services)
